{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ACGPN.predict_pose import generate_pose_keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPENPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openpose_done(image_dir):\n",
    "    images = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "    # 各画像に対してポーズキーポイントを生成\n",
    "    for img in images:\n",
    "        img_filename = os.path.basename(img)\n",
    "        pose_path = os.path.join('./experiment/output/json',img_filename.replace('.jpg','_keypoints.json'))\n",
    "        print(img, pose_path)\n",
    "        generate_pose_keypoints(img, pose_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCHP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "#image_seg = \"./Data_preprocessing/test_label/\"\n",
    "input_dir = './kosei_takaki/DeepFashion_Try_On/Data_preprocessing/test_img'\n",
    "output_dir = \"./kosei_takaki/DeepFashion_Try_On/Data_preprocessing/test_label\"\n",
    "subprocess.run([\"python3\", \"./Self-Correction-Human-Parsing-for-ACGPN/simple_extractor.py\", '--dataset', 'lip', '--model-restore', './Self-Correction-Human-Parsing-for-ACGPN/final.pth', '--input-dir', input_dir, '--output-dir', output_dir], stdout=PIPE, stderr=PIPE)\n",
    "#image = cv2.imread(output_dir)python3 Self-Correction-Human-Parsing-for-ACGPN/simple_extractor.py --dataset 'lip' --model-restore 'lip_final.pth' --input-dir '/content/drive/MyDrive/pi' --output-dir '/content/drive/MyDrive/ip'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
